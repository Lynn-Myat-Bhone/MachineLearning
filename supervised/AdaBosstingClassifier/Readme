<h3>AdaBoosting</h3>
<hr>
<p>AdaBoost, short for Adaptive Boosting, is a popular ensemble learning technique that focuses on 
    improving the performance of weak learners (often decision trees with a single split, called decision stumps) 
    by combining them into a strong classifier
</p>
<p>Algorithm Workflow:
    Initialize Weights:
    
    Start by assigning equal weights to all the data points.
    Train Weak Classifier:
    
    Train the first weak learner on the training data.
    Evaluate Errors:
    
    Calculate the error rate of the weak learner.
    Adjust Weights:
    
    Increase the weights of misclassified samples and decrease the weights of correctly classified samples.
    Update Model:
    
    Combine the weak learner into the overall model and repeat the process for a fixed number of iterations or until the desired accuracy is reached.
</p>